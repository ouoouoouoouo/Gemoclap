"""
GEmo-CLAP è¨“ç·´è…³æœ¬
"""

import os
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR  # â­ æ·»åŠ é€™è¡Œ
from transformers import AutoTokenizer
from tqdm import tqdm
import json

from config import Config
from dataset import (
    IEMOCAPDataset, 
    load_iemocap_data, 
    create_fold_splits,
    collate_fn
)
from models import create_model
from evaluate import evaluate_model


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def train_one_epoch(model, dataloader, optimizer, device, model_type):
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()
    total_loss = 0
    accumulation_steps = Config.GRADIENT_ACCUMULATION_STEPS
    
    pbar = tqdm(dataloader, desc="è¨“ç·´ä¸­")
    optimizer.zero_grad()
    
    # ç´¯ç©å¤šå€‹ mini-batch çš„ embeddingï¼Œå½¢æˆæ›´å¤§çš„å°æ¯” batch
    accum_audio_embeddings = []
    accum_text_embeddings = []
    accum_emotion_labels = []
    accum_gender_labels = []
    
    for batch_idx, batch in enumerate(pbar):
        # ç§»å‹•æ•¸æ“šåˆ°è¨­å‚™
        audio_input = {k: v.to(device) for k, v in batch["audio_input"].items()}
        text_input = {k: v.to(device) for k, v in batch["text_input"].items()}
        emotion_labels = batch["emotion_label"].to(device)
        gender_labels = batch["gender_label"].to(device)
        
        # å‰å‘å‚³æ’­
        audio_embeddings, text_embeddings = model(audio_input, text_input)

        # ç´¯ç© embeddings èˆ‡ labelsï¼ˆä¿æŒè¨ˆç®—åœ–ï¼Œå½¢æˆæ›´å¤§å°æ¯” batchï¼‰
        accum_audio_embeddings.append(audio_embeddings)
        accum_text_embeddings.append(text_embeddings)
        accum_emotion_labels.append(emotion_labels)
        accum_gender_labels.append(gender_labels)
        
        is_update_step = (batch_idx + 1) % accumulation_steps == 0
        is_last_step = (batch_idx + 1) == len(dataloader)
        
        if is_update_step or is_last_step:
            # åˆä½µç‚ºå¤§ batch
            audio_batch = torch.cat(accum_audio_embeddings, dim=0)
            text_batch = torch.cat(accum_text_embeddings, dim=0)
            emotion_batch = torch.cat(accum_emotion_labels, dim=0)
            gender_batch = torch.cat(accum_gender_labels, dim=0)
            
            # è™•ç† DataParallel åŒ…è£çš„æ¨¡å‹
            actual_model = model.module if isinstance(model, nn.DataParallel) else model
            
            # è¨ˆç®—æå¤±ï¼ˆå°æ¯”å­¸ç¿’éœ€è¦å¤§ batchï¼‰
            if model_type == "emo_clap":
                loss = actual_model.compute_contrastive_loss(
                    audio_batch, text_batch, emotion_batch
                )
            elif model_type == "ml_gemo_clap":
                loss = actual_model.compute_multitask_loss(
                    audio_batch, text_batch,
                    emotion_batch, gender_batch
                )
            elif model_type == "sl_gemo_clap":
                loss = actual_model.compute_soft_label_loss(
                    audio_batch, text_batch,
                    emotion_batch, gender_batch
                )
            else:
                raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é¡å‹: {model_type}")
            
            # åå‘å‚³æ’­èˆ‡æ›´æ–°
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
            pbar.set_postfix({
                "loss": f"{loss.item():.4f}",
                "step": f"{(batch_idx + 1) // accumulation_steps}/{len(dataloader) // accumulation_steps}"
            })
            
            # æ¸…ç©ºç´¯ç©
            accum_audio_embeddings.clear()
            accum_text_embeddings.clear()
            accum_emotion_labels.clear()
            accum_gender_labels.clear()
    
    # å¹³å‡ loss ä»¥æ›´æ–°æ­¥æ•¸è¨ˆ
    update_steps = (len(dataloader) + accumulation_steps - 1) // accumulation_steps
    avg_loss = total_loss / max(update_steps, 1)
    return avg_loss


def train_fold(model, train_loader, val_loader, optimizer, scheduler, device,  # â­ æ·»åŠ  scheduler
               num_epochs, model_type, save_dir, fold_num, patience=80):
    """
    è¨“ç·´ä¸€å€‹ fold
    
    Args:
        scheduler: å­¸ç¿’ç‡èª¿åº¦å™¨
        patience: Early stopping çš„è€å¿ƒå€¼
    """
    best_uar = 0
    best_epoch = 0
    epochs_no_improve = 0
    history = {
        "train_loss": [],
        "val_war": [],
        "val_uar": [],
        "learning_rate": []  # â­ è¨˜éŒ„å­¸ç¿’ç‡
    }
    
    for epoch in range(num_epochs):
        print(f"\n{'='*80}")
        print(f"Fold {fold_num} - Epoch {epoch+1}/{num_epochs}")
        print(f"{'='*80}")
        
        # â­ æ‰“å°ç•¶å‰å­¸ç¿’ç‡
        current_lr = optimizer.param_groups[0]['lr']
        print(f"ç•¶å‰å­¸ç¿’ç‡: {current_lr:.2e}")
        
        # è¨“ç·´
        train_loss = train_one_epoch(model, train_loader, optimizer, device, model_type)
        print(f"è¨“ç·´æå¤±: {train_loss:.4f}")
        
        # é©—è­‰
        val_metrics = evaluate_model(model, val_loader, device)
        val_war = val_metrics["WAR"]
        val_uar = val_metrics["UAR"]
        
        print(f"é©—è­‰ WAR: {val_war:.2f}%")
        print(f"é©—è­‰ UAR: {val_uar:.2f}%")
        
        # ä¿å­˜æ­·å²
        history["train_loss"].append(train_loss)
        history["val_war"].append(val_war)
        history["val_uar"].append(val_uar)
        history["learning_rate"].append(current_lr)  # â­ è¨˜éŒ„å­¸ç¿’ç‡
        
        # â­ æ›´æ–°å­¸ç¿’ç‡ï¼ˆåœ¨ epoch çµæŸæ™‚ï¼‰
        scheduler.step()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_uar > best_uar:
            best_uar = val_uar
            best_epoch = epoch + 1
            epochs_no_improve = 0
            
            if save_dir:
                os.makedirs(save_dir, exist_ok=True)
                save_path = os.path.join(save_dir, f"fold_{fold_num}_best.pth")
                torch.save({
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "scheduler_state_dict": scheduler.state_dict(),  # â­ ä¿å­˜ scheduler
                    "uar": best_uar,
                    "war": val_war
                }, save_path)
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {save_path} (UAR: {best_uar:.2f}%)")
        else:
            epochs_no_improve += 1
            print(f"âš ï¸  UAR æ²’æœ‰æå‡ ({epochs_no_improve}/{patience})")
            
            # Early Stopping
            if epochs_no_improve >= patience:
                print(f"\nğŸ›‘ Early Stopping: UAR åœ¨ {patience} å€‹ epoch å…§æ²’æœ‰æå‡")
                print(f"   æœ€ä½³ UAR: {best_uar:.2f}% (Epoch {best_epoch})")
                break
    
    print(f"\nFold {fold_num} æœ€ä½³çµæœ: UAR={best_uar:.2f}% (Epoch {best_epoch})")
    
    return best_uar, history


def main(args):
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    set_seed(Config.SEED)
    
    # è¨­ç½®è¨­å‚™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n{'='*60}")
    print(f"è¨­å‚™é…ç½®")
    print(f"{'='*60}")
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"GPU å‹è™Ÿ: {gpu_name}")
        print(f"GPU é¡¯å­˜: {gpu_memory:.2f} GB")
        print(f"GPU æ•¸é‡: {torch.cuda.device_count()}")
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"âœ… å°‡ä½¿ç”¨ GPU åŠ é€Ÿè¨“ç·´")
    else:
        print(f"âš ï¸  è­¦å‘Š: CUDA ä¸å¯ç”¨")
    print(f"{'='*60}\n")
    
    # è¼‰å…¥æ•¸æ“š
    print("\nè¼‰å…¥ IEMOCAP æ•¸æ“šé›†...")
    data_list = load_iemocap_data(Config.IEMOCAP_PATH)
    
    # å‰µå»º fold åˆ†å‰²
    print("\nå‰µå»º 5-fold äº¤å‰é©—è­‰åˆ†å‰²...")
    folds = create_fold_splits(data_list, n_folds=Config.N_FOLDS)
    
    # åˆå§‹åŒ–è™•ç†å™¨
    print("\nè¼‰å…¥é è¨“ç·´æ¨¡å‹å’Œè™•ç†å™¨...")
    text_processor = AutoTokenizer.from_pretrained(Config.TEXT_ENCODER)
    
    from transformers import Wav2Vec2FeatureExtractor
    audio_encoder_path = Config.AUDIO_ENCODERS[args.audio_encoder]
    audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(audio_encoder_path)
    
    # å­˜å„²æ¯å€‹ fold çš„çµæœ
    fold_results = []
    
    # è¨“ç·´æ¯å€‹ fold
    for fold_num, (train_data, test_data) in enumerate(folds, 1):
        print(f"\n{'#'*60}")
        print(f"é–‹å§‹è¨“ç·´ Fold {fold_num}/{Config.N_FOLDS}")
        print(f"{'#'*60}")
        
        # å‰µå»ºæ•¸æ“šé›†
        train_dataset = IEMOCAPDataset(
            train_data, 
            audio_processor=audio_processor,
            text_processor=text_processor,
            mode='train'
        )
        test_dataset = IEMOCAPDataset(
            test_data,
            audio_processor=audio_processor,
            text_processor=text_processor,
            mode='test'
        )
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=Config.BATCH_SIZE,
            shuffle=True,
            num_workers=Config.NUM_WORKERS,
            collate_fn=collate_fn,
            pin_memory=True if torch.cuda.is_available() else False
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=Config.BATCH_SIZE,
            shuffle=False,
            num_workers=Config.NUM_WORKERS,
            collate_fn=collate_fn,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        # å‰µå»ºæ¨¡å‹
        model = create_model(
            model_type=args.model_type,
            text_encoder=Config.TEXT_ENCODER,
            audio_encoder=args.audio_encoder,
            embedding_dim=Config.EMBEDDING_DIM,
            alpha_e=Config.ALPHA_E if args.model_type != "emo_clap" else None
        )
        
        # å¤š GPU æ”¯æŒ
        if torch.cuda.device_count() > 1:
            print(f"ä½¿ç”¨ {torch.cuda.device_count()} å¼µ GPU")
            model = nn.DataParallel(model)
        
        model = model.to(device)
        
        # â­ å‰µå»ºå„ªåŒ–å™¨
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=Config.LEARNING_RATE
        )
        
        # â­ å‰µå»ºå­¸ç¿’ç‡èª¿åº¦å™¨
        scheduler = CosineAnnealingLR(
            optimizer, 
            T_max=args.epochs,      # ç¸½ epoch æ•¸
            eta_min=1e-6            # æœ€å°å­¸ç¿’ç‡
        )
        
        print(f"\nğŸ“Š è¨“ç·´è¨­ç½®:")
        print(f"  åˆå§‹å­¸ç¿’ç‡: {Config.LEARNING_RATE:.2e}")
        print(f"  æœ€å°å­¸ç¿’ç‡: 1e-6")
        print(f"  èª¿åº¦å™¨: CosineAnnealingLR")
        print(f"  ç¸½ epochs: {args.epochs}")
        
        # â­ è¨“ç·´ï¼ˆå‚³å…¥ schedulerï¼‰
        best_uar, history = train_fold(
            model, train_loader, test_loader, optimizer, scheduler, device,  # å‚³å…¥ scheduler
            num_epochs=args.epochs,
            model_type=args.model_type,
            save_dir=args.save_dir,
            fold_num=fold_num
        )
        
        fold_results.append({
            "fold": fold_num,
            "best_uar": best_uar,
            "history": history
        })
    
    # è¨ˆç®—å¹³å‡çµæœ
    avg_uar = np.mean([r["best_uar"] for r in fold_results])
    std_uar = np.std([r["best_uar"] for r in fold_results])
    
    print(f"\n{'='*60}")
    print(f"5-Fold äº¤å‰é©—è­‰çµæœ")
    print(f"{'='*60}")
    print(f"å¹³å‡ UAR: {avg_uar:.2f}% (Â±{std_uar:.2f}%)")
    
    for result in fold_results:
        print(f"  Fold {result['fold']}: UAR={result['best_uar']:.2f}%")
    
    # ä¿å­˜çµæœ
    if args.save_dir:
        results_path = os.path.join(args.save_dir, "results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump({
                "model_type": args.model_type,
                "audio_encoder": args.audio_encoder,
                "avg_uar": float(avg_uar),
                "std_uar": float(std_uar),
                "fold_results": fold_results
            }, f, indent=2, ensure_ascii=False)
        print(f"\nçµæœå·²ä¿å­˜åˆ° {results_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¨“ç·´ GEmo-CLAP æ¨¡å‹")
    
    parser.add_argument(
        "--model_type",
        type=str,
        default="sl_gemo_clap",
        choices=["emo_clap", "ml_gemo_clap", "sl_gemo_clap"],
        help="æ¨¡å‹é¡å‹"
    )
    parser.add_argument(
        "--audio_encoder",
        type=str,
        default="wavlm",
        choices=["wav2vec2", "hubert", "wavlm", "wavlm-base", "data2vec"],  # â­ æ·»åŠ  wavlm
        help="éŸ³é »ç·¨ç¢¼å™¨é¡å‹"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=80,
        help="è¨“ç·´è¼ªæ•¸"
    )
    parser.add_argument(
        "--save_dir",
        type=str,
        default="checkpoints",
        help="æ¨¡å‹ä¿å­˜ç›®éŒ„"
    )
    
    args = parser.parse_args()
    
    print(f"\nè¨“ç·´é…ç½®:")
    print(f"  æ¨¡å‹é¡å‹: {args.model_type}")
    print(f"  éŸ³é »ç·¨ç¢¼å™¨: {args.audio_encoder}")
    print(f"  è¨“ç·´è¼ªæ•¸: {args.epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {Config.BATCH_SIZE}")
    print(f"  å­¸ç¿’ç‡: {Config.LEARNING_RATE}")
    
    main(args)